{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.8 64-bit",
   "display_name": "Python 3.7.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "9a139ee91aa0c0c703de3f6d61738888479023eb2dcd0ff79f15c99def10401a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "embed_dim = 50\n",
    "epochs = 1_000\n",
    "\n",
    "n_lines = 2809381\n",
    "data_path = Path(\"..\") / \"data\"\n",
    "warmstarted_embeddings_path = data_path/\"warm-started-embeddings\"/f\"glove.6B.en-pt.{embed_dim}d.txt.sample\"\n",
    "\n",
    "paracrawl_crosslingual_all_data_path = data_path / \"parallel\" / \"paracrawl.en-pt\" / \"paracrawl.crosslingual.en-pt.all.sample\"\n",
    "\n",
    "train_log_path = data_path / \"train_logs\" / f\"loss_warm_started_{embed_dim}d_{epochs}epochs_{int(time.time())}.txt\"\n",
    "checkpoints_path = data_path / \"checkpoints\"\n",
    "results_path = data_path / \"results\" / f\"word2vec_warmstarted_trained_embeddings_{embed_dim}d_{epochs}epochs.txt\"\n",
    "\n",
    "remove_punct_regex = re.compile(r\"[^\\w\\s]\")\n",
    "remove_punct = partial(remove_punct_regex.sub, repl = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpoint(CallbackAny2Vec):\n",
    "    def __init__(self, checkpoints_path):\n",
    "        self.epoch = 0\n",
    "        self.checkpoints_path = checkpoints_path\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        if (self.epoch + 1) % 10 == 0:\n",
    "            filepath = self.checkpoints_path/f\"word2vec-warmstart-{model.trainables.layer1_size}d-epoch{self.epoch + 1}.model\"\n",
    "            with open(filepath, \"w\") as f:\n",
    "                filename = f.name\n",
    "                model.save(filename)\n",
    "            \n",
    "            model.wv.save_word2vec_format(str(results_path))\n",
    "\n",
    "class LossLogger(CallbackAny2Vec):\n",
    "    def __init__(self, train_log_path):\n",
    "        self.epoch = 0\n",
    "        self.batch = 0\n",
    "        self.train_log_path = train_log_path\n",
    "\n",
    "    def on_train_begin(self, model):\n",
    "        print(\"[ ] Starting Word2Vec training...\")\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_train_end(self, model):\n",
    "        print(f\"[ ] Word2Vec finished training in {time.time() - self.start_time} seconds\")\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_start(self, model):\n",
    "        self.batch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        print(f'[ ] Loss after epoch {self.epoch}: {loss}')\n",
    "\n",
    "        with open(self.train_log_path, \"a\") as f:\n",
    "            f.write(str(loss) + \"\\n\")\n",
    "        \n",
    "        self.epoch = self.epoch + 1\n",
    "\n",
    "\n",
    "def preprocess_line(line):\n",
    "    return remove_punct(string = line.replace(\"\\n\", \"\").lower()).split()\n",
    "\n",
    "def read_corpus(files, shuffle_lines = False):\n",
    "    sentences = []\n",
    "    for file_path in files:\n",
    "        print(f\"[ ] Reading file {file_path}\")\n",
    "        time.sleep(1)\n",
    "        with open(file_path, 'r', encoding = 'utf8') as f:\n",
    "            for line in tqdm(f, total = n_lines):\n",
    "                sentences.append((preprocess_line(line)))\n",
    "\n",
    "    if shuffle_lines:\n",
    "        np.random.shuffle(sentences)\n",
    "    return sentences\n",
    "\n",
    "def single_file_corpus(file):\n",
    "    with open(file, 'r', encoding = 'utf8') as f:\n",
    "        for line in tqdm(f, total = 4*n_lines):\n",
    "            yield preprocess_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmstart_embeddings = KeyedVectors.load_word2vec_format(warmstarted_embeddings_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ ] Starting Word2Vec training...\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "total_words must be provided alongside corpus_file argument.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-b0f047410304>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwarmstart_embeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersect_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwarmstarted_embeddings_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlockf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparacrawl_crosslingual_all_data_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1065\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1067\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n\u001b[1;32m--> 555\u001b[1;33m                     corpus_file, cur_epoch=cur_epoch, total_examples=total_examples, total_words=total_words, **kwargs)\n\u001b[0m\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[0mtrained_word_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrained_word_count_epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch_corpusfile\u001b[1;34m(self, corpus_file, cur_epoch, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m         \"\"\"\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"total_words must be provided alongside corpus_file argument.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2vec_corpusfile\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCythonVocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: total_words must be provided alongside corpus_file argument."
     ]
    }
   ],
   "source": [
    "model = Word2Vec(size = embed_dim, \n",
    "                     iter = 10,\n",
    "                     min_count = 1, \n",
    "                     compute_loss = True, \n",
    "                     callbacks = [ModelCheckpoint(checkpoints_path), LossLogger(train_log_path)]\n",
    "                    )\n",
    "# corpus = list(single_file_corpus(paracrawl_crosslingual_all_data_path))\n",
    "model.build_vocab(corpus_file = paracrawl_crosslingual_all_data_path)\n",
    "total_examples = model.corpus_count\n",
    "model.build_vocab([list(warmstart_embeddings.vocab.keys())], update=True)\n",
    "model.intersect_word2vec_format(warmstarted_embeddings_path, binary=False, lockf=1.0)\n",
    "model.train(corpus_file = paracrawl_crosslingual_all_data_path, total_examples = total_examples, epochs=model.epochs, compute_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "  0%|          | 10000/11237524 [00:00<01:23, 135042.24it/s]\n[ ] Starting Word2Vec training...\n[ ] Loss after epoch 0: 215512.0625\n[ ] Loss after epoch 1: 401047.5\n[ ] Loss after epoch 2: 573787.5\n[ ] Loss after epoch 3: 738846.3125\n[ ] Loss after epoch 4: 896090.9375\n[ ] Loss after epoch 5: 1029771.0625\n[ ] Loss after epoch 6: 1176073.375\n[ ] Loss after epoch 7: 1318633.0\n[ ] Loss after epoch 8: 1459131.25\n[ ] Loss after epoch 9: 1598792.25\n[ ] Word2Vec finished training in 1.4460091590881348 seconds\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2210060, 2210060)"
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "model = Word2Vec(size = embed_dim, \n",
    "                     iter = 10,\n",
    "                     min_count = 1, \n",
    "                     compute_loss = True, \n",
    "                     callbacks = [ModelCheckpoint(checkpoints_path), LossLogger(train_log_path)]\n",
    "                    )\n",
    "corpus = list(single_file_corpus(paracrawl_crosslingual_all_data_path))\n",
    "model.build_vocab(corpus)\n",
    "total_examples = model.corpus_count\n",
    "model.build_vocab([list(warmstart_embeddings.vocab.keys())], update=True)\n",
    "model.intersect_word2vec_format(warmstarted_embeddings_path, binary=False, lockf=1.0)\n",
    "model.train(corpus, total_examples = total_examples, epochs=model.epochs, compute_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ ] Initializing model...\n"
    }
   ],
   "source": [
    "print(\"[ ] Initializing model...\")\n",
    "model = Word2Vec(size=embed_dim, \n",
    "                 iter = 11,\n",
    "                 min_count=1, \n",
    "                 compute_loss = True, \n",
    "                 callbacks=[ModelCheckpoint(checkpoints_path), LossLogger(train_log_path)]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ ] Loading warmstarted embeddings...\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2323"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "print(\"[ ] Loading warmstarted embeddings...\")\n",
    "warmstart_embeddings = KeyedVectors.load_word2vec_format(warmstarted_embeddings_path, binary=False)\n",
    "len(warmstart_embeddings.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ ] Building vocab...\n  0%|          | 10000/11237524 [00:00<02:16, 82133.03it/s]\n"
    }
   ],
   "source": [
    "print(\"[ ] Building vocab...\")\n",
    "time.sleep(1)\n",
    "corpus = single_file_corpus(paracrawl_crosslingual_all_data_path)\n",
    "model.build_vocab(corpus)\n",
    "total_examples = model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ ] Updating vocab...\n"
    }
   ],
   "source": [
    "print(\"[ ] Updating vocab...\")\n",
    "model.build_vocab([list(warmstart_embeddings.vocab.keys())], update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ ] Warm-starting embeddings...\n"
    }
   ],
   "source": [
    "print(\"[ ] Warm-starting embeddings...\")\n",
    "model.intersect_word2vec_format(warmstarted_embeddings_path, binary=False, lockf=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ ] Starting Word2Vec training...\n[ ] Loss after epoch 0: 0\n[ ] Loss after epoch 1: 0\n[ ] Loss after epoch 2: 0\n[ ] Loss after epoch 3: 0\n[ ] Loss after epoch 4: 0\n[ ] Loss after epoch 5: 0\n[ ] Loss after epoch 6: 0\n[ ] Loss after epoch 7: 0\n[ ] Loss after epoch 8: 0\n[ ] Loss after epoch 9: 0\n[ ] Loss after epoch 10: 0\n[ ] Word2Vec finished training in 0.04525399208068848 seconds\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(0, 0)"
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "model.train(corpus, total_examples=total_examples, epochs=model.iter, compute_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec?"
   ]
  }
 ]
}